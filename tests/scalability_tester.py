#!/usr/bin/env python3
"""
Scalability Testing Framework cho Microservices Deployment Orchestrator

Framework n√†y test hi·ªáu nƒÉng, t·∫£i v√† kh·∫£ nƒÉng scale c·ªßa h·ªá th·ªëng deployment.
"""

import asyncio
import time
import json
import statistics
import boto3
import concurrent.futures
from datetime import datetime, timezone
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import click
import yaml
from tabulate import tabulate
import threading


@dataclass
class TestResult:
    """K·∫øt qu·∫£ c·ªßa m·ªôt test case"""
    test_name: str
    start_time: datetime
    end_time: datetime
    duration: float
    success: bool
    error_message: Optional[str] = None
    metrics: Dict = None


@dataclass
class LoadTestConfig:
    """C·∫•u h√¨nh cho load testing"""
    concurrent_deployments: int
    total_deployments: int
    deployment_interval: float  # seconds
    timeout_per_deployment: int  # seconds
    services_per_deployment: List[str]
    environment: str = "load-test"


class ScalabilityTester:
    """Class ch√≠nh cho scalability testing"""
    
    def __init__(self, config_file: str = "config/aws_config.yaml"):
        """Kh·ªüi t·∫°o scalability tester"""
        self.config = self.load_config(config_file)
        self.setup_aws_clients()
        self.test_results: List[TestResult] = []
        
    def load_config(self, config_file: str) -> Dict:
        """Load AWS config"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            raise ValueError(f"Error loading config: {str(e)}")
    
    def setup_aws_clients(self):
        """Setup AWS clients"""
        try:
            session = boto3.Session(
                profile_name=self.config['aws'].get('profile', 'default'),
                region_name=self.config['aws']['region']
            )
            
            self.stepfunctions_client = session.client('stepfunctions')
            self.cloudwatch_client = session.client('cloudwatch')
            self.lambda_client = session.client('lambda')
            
            print("‚úÖ AWS clients initialized for scalability testing")
            
        except Exception as e:
            raise Exception(f"Error setting up AWS clients: {str(e)}")
    
    def get_state_machine_arn(self) -> str:
        """L·∫•y ARN c·ªßa state machine"""
        state_machine_name = self.config['step_functions']['state_machine_name']
        account_id = self.get_account_id()
        region = self.config['aws']['region']
        
        return f"arn:aws:states:{region}:{account_id}:stateMachine:{state_machine_name}"
    
    def get_account_id(self) -> str:
        """L·∫•y AWS Account ID"""
        try:
            sts_client = boto3.client('sts')
            response = sts_client.get_caller_identity()
            return response['Account']
        except Exception:
            return "123456789012"  # Fallback
    
    def run_performance_baseline_test(self) -> TestResult:
        """Test hi·ªáu nƒÉng c∆° b·∫£n v·ªõi 1 deployment"""
        test_name = "Performance Baseline"
        start_time = datetime.now(timezone.utc)
        
        try:
            print(f"üèÉ Running {test_name}...")
            
            # Single deployment
            execution_input = {
                "environment": "performance-test",
                "version": "baseline-test",
                "services": ["auth-service"],
                "timestamp": start_time.isoformat(),
                "test_mode": True
            }
            
            response = self.stepfunctions_client.start_execution(
                stateMachineArn=self.get_state_machine_arn(),
                name=f"perf-baseline-{int(time.time())}",
                input=json.dumps(execution_input)
            )
            
            execution_arn = response['executionArn']
            
            # ƒê·ª£i ho√†n th√†nh
            result = self._wait_for_execution(execution_arn, timeout=300)
            
            end_time = datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()
            
            success = result['status'] == 'SUCCEEDED'
            
            metrics = {
                "execution_arn": execution_arn,
                "execution_time": duration,
                "status": result['status']
            }
            
            return TestResult(
                test_name=test_name,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                success=success,
                metrics=metrics
            )
            
        except Exception as e:
            end_time = datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()
            
            return TestResult(
                test_name=test_name,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                success=False,
                error_message=str(e)
            )
    
    def run_concurrent_deployment_test(self, concurrent_count: int = 5) -> TestResult:
        """Test concurrent deployments"""
        test_name = f"Concurrent Deployments (x{concurrent_count})"
        start_time = datetime.now(timezone.utc)
        
        try:
            print(f"‚ö° Running {test_name}...")
            
            # T·∫°o concurrent executions
            execution_futures = []
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_count) as executor:
                for i in range(concurrent_count):
                    future = executor.submit(self._single_deployment_test, i)
                    execution_futures.append(future)
                
                # ƒê·ª£i t·∫•t c·∫£ ho√†n th√†nh
                results = []
                for future in concurrent.futures.as_completed(execution_futures, timeout=600):
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        results.append({"success": False, "error": str(e)})
            
            end_time = datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()
            
            # Ph√¢n t√≠ch k·∫øt qu·∫£
            successful = sum(1 for r in results if r.get("success", False))
            failed = len(results) - successful
            
            execution_times = [r.get("duration", 0) for r in results if r.get("success", False)]
            avg_execution_time = statistics.mean(execution_times) if execution_times else 0
            
            metrics = {
                "concurrent_count": concurrent_count,
                "total_executions": len(results),
                "successful": successful,
                "failed": failed,
                "success_rate": successful / len(results) * 100,
                "avg_execution_time": avg_execution_time,
                "max_execution_time": max(execution_times) if execution_times else 0,
                "min_execution_time": min(execution_times) if execution_times else 0
            }
            
            return TestResult(
                test_name=test_name,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                success=successful > 0,
                metrics=metrics
            )
            
        except Exception as e:
            end_time = datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()
            
            return TestResult(
                test_name=test_name,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                success=False,
                error_message=str(e)
            )
    
    def _single_deployment_test(self, test_id: int) -> Dict:
        """Single deployment test cho concurrent testing"""
        try:
            start_time = datetime.now(timezone.utc)
            
            execution_input = {
                "environment": f"concurrent-test-{test_id}",
                "version": "concurrent-test",
                "services": ["auth-service"],
                "timestamp": start_time.isoformat(),
                "test_mode": True,
                "test_id": test_id
            }
            
            response = self.stepfunctions_client.start_execution(
                stateMachineArn=self.get_state_machine_arn(),
                name=f"concurrent-test-{test_id}-{int(time.time())}",
                input=json.dumps(execution_input)
            )
            
            execution_arn = response['executionArn']
            
            # ƒê·ª£i ho√†n th√†nh
            result = self._wait_for_execution(execution_arn, timeout=300)
            
            end_time = datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()
            
            return {
                "success": result['status'] == 'SUCCEEDED',
                "duration": duration,
                "execution_arn": execution_arn,
                "test_id": test_id,
                "status": result['status']
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "test_id": test_id
            }
    
    def run_load_test(self, config: LoadTestConfig) -> TestResult:
        """Ch·∫°y load test v·ªõi c·∫•u h√¨nh cho tr∆∞·ªõc"""
        test_name = f"Load Test ({config.concurrent_deployments} concurrent, {config.total_deployments} total)"
        start_time = datetime.now(timezone.utc)
        
        try:
            print(f"üî• Running {test_name}...")
            
            # Ph√¢n chia deployments th√†nh batches
            batches = []
            remaining = config.total_deployments
            
            while remaining > 0:
                batch_size = min(config.concurrent_deployments, remaining)
                batches.append(batch_size)
                remaining -= batch_size
            
            all_results = []
            
            for batch_num, batch_size in enumerate(batches):
                print(f"  üöÄ Batch {batch_num + 1}: {batch_size} deployments")
                
                # Concurrent deployments trong batch
                with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
                    batch_futures = []
                    
                    for i in range(batch_size):
                        deployment_id = batch_num * config.concurrent_deployments + i
                        future = executor.submit(self._load_test_deployment, deployment_id, config)
                        batch_futures.append(future)
                    
                    # Collect results
                    for future in concurrent.futures.as_completed(batch_futures, timeout=config.timeout_per_deployment):
                        try:
                            result = future.result()
                            all_results.append(result)
                        except Exception as e:
                            all_results.append({"success": False, "error": str(e)})
                
                # Interval between batches
                if batch_num < len(batches) - 1:  # Kh√¥ng sleep sau batch cu·ªëi
                    time.sleep(config.deployment_interval)
            
            end_time = datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()
            
            # Ph√¢n t√≠ch k·∫øt qu·∫£
            successful = sum(1 for r in all_results if r.get("success", False))
            failed = len(all_results) - successful
            
            execution_times = [r.get("duration", 0) for r in all_results if r.get("success", False)]
            
            metrics = {
                "total_deployments": config.total_deployments,
                "concurrent_deployments": config.concurrent_deployments,
                "successful": successful,
                "failed": failed,
                "success_rate": successful / len(all_results) * 100 if all_results else 0,
                "avg_execution_time": statistics.mean(execution_times) if execution_times else 0,
                "max_execution_time": max(execution_times) if execution_times else 0,
                "min_execution_time": min(execution_times) if execution_times else 0,
                "total_test_time": duration,
                "deployments_per_minute": len(all_results) / (duration / 60) if duration > 0 else 0
            }
            
            return TestResult(
                test_name=test_name,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                success=successful > 0,
                metrics=metrics
            )
            
        except Exception as e:
            end_time = datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()
            
            return TestResult(
                test_name=test_name,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                success=False,
                error_message=str(e)
            )
    
    def _load_test_deployment(self, deployment_id: int, config: LoadTestConfig) -> Dict:
        """Single deployment cho load test"""
        try:
            start_time = datetime.now(timezone.utc)
            
            execution_input = {
                "environment": config.environment,
                "version": f"load-test-{deployment_id}",
                "services": config.services_per_deployment,
                "timestamp": start_time.isoformat(),
                "test_mode": True,
                "deployment_id": deployment_id
            }
            
            response = self.stepfunctions_client.start_execution(
                stateMachineArn=self.get_state_machine_arn(),
                name=f"load-test-{deployment_id}-{int(time.time())}",
                input=json.dumps(execution_input)
            )
            
            execution_arn = response['executionArn']
            
            # ƒê·ª£i ho√†n th√†nh
            result = self._wait_for_execution(execution_arn, timeout=config.timeout_per_deployment)
            
            end_time = datetime.now(timezone.utc)
            duration = (end_time - start_time).total_seconds()
            
            return {
                "success": result['status'] == 'SUCCEEDED',
                "duration": duration,
                "execution_arn": execution_arn,
                "deployment_id": deployment_id,
                "status": result['status']
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "deployment_id": deployment_id
            }
    
    def _wait_for_execution(self, execution_arn: str, timeout: int = 600) -> Dict:
        """ƒê·ª£i execution ho√†n th√†nh"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                response = self.stepfunctions_client.describe_execution(
                    executionArn=execution_arn
                )
                
                status = response['status']
                
                if status in ['SUCCEEDED', 'FAILED', 'ABORTED']:
                    return {
                        'status': status,
                        'start_date': response['startDate'],
                        'stop_date': response.get('stopDate'),
                        'execution_arn': execution_arn
                    }
                
                time.sleep(2)  # Check every 2 seconds
                
            except Exception as e:
                return {
                    'status': 'ERROR',
                    'error': str(e),
                    'execution_arn': execution_arn
                }
        
        return {
            'status': 'TIMEOUT',
            'execution_arn': execution_arn
        }
    
    def run_stress_test(self, max_concurrent: int = 20, step: int = 5) -> List[TestResult]:
        """Stress test - tƒÉng d·∫ßn concurrent deployments"""
        results = []
        
        print(f"üí™ Running Stress Test (up to {max_concurrent} concurrent)")
        
        for concurrent in range(step, max_concurrent + 1, step):
            print(f"\nüîÑ Testing {concurrent} concurrent deployments...")
            
            result = self.run_concurrent_deployment_test(concurrent)
            results.append(result)
            
            # Break n·∫øu success rate qu√° th·∫•p
            if result.metrics and result.metrics.get('success_rate', 0) < 50:
                print(f"‚ö†Ô∏è  Success rate dropped below 50% at {concurrent} concurrent deployments")
                break
        
        return results
    
    def run_full_test_suite(self) -> List[TestResult]:
        """Ch·∫°y to√†n b·ªô test suite"""
        print("üß™ Running Full Scalability Test Suite")
        print("="*60)
        
        all_results = []
        
        # 1. Performance Baseline
        result = self.run_performance_baseline_test()
        all_results.append(result)
        
        # 2. Concurrent Tests
        for concurrent in [3, 5, 10]:
            result = self.run_concurrent_deployment_test(concurrent)
            all_results.append(result)
        
        # 3. Load Test
        load_config = LoadTestConfig(
            concurrent_deployments=5,
            total_deployments=20,
            deployment_interval=2.0,
            timeout_per_deployment=300,
            services_per_deployment=["auth-service"]
        )
        
        result = self.run_load_test(load_config)
        all_results.append(result)
        
        # 4. Stress Test
        stress_results = self.run_stress_test(max_concurrent=15, step=3)
        all_results.extend(stress_results)
        
        self.test_results = all_results
        return all_results
    
    def generate_report(self, results: List[TestResult] = None) -> str:
        """T·∫°o b√°o c√°o scalability testing"""
        if results is None:
            results = self.test_results
        
        report = []
        report.append("="*80)
        report.append("üöÄ SCALABILITY TESTING REPORT")
        report.append("="*80)
        report.append(f"Test Date: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}")
        report.append(f"Total Tests: {len(results)}")
        
        successful_tests = sum(1 for r in results if r.success)
        report.append(f"Successful Tests: {successful_tests}/{len(results)} ({successful_tests/len(results)*100:.1f}%)")
        report.append("")
        
        # Test Results Table
        table_data = []
        for result in results:
            row = [
                result.test_name,
                "‚úÖ" if result.success else "‚ùå",
                f"{result.duration:.1f}s",
                result.error_message or "N/A"
            ]
            table_data.append(row)
        
        headers = ["Test Name", "Status", "Duration", "Error"]
        table = tabulate(table_data, headers=headers, tablefmt="grid")
        report.append(table)
        report.append("")
        
        # Detailed Metrics
        report.append("üìä DETAILED METRICS")
        report.append("-" * 40)
        
        for result in results:
            if result.metrics:
                report.append(f"\nüîç {result.test_name}:")
                for key, value in result.metrics.items():
                    if isinstance(value, float):
                        report.append(f"  {key}: {value:.2f}")
                    else:
                        report.append(f"  {key}: {value}")
        
        # Performance Summary
        report.append("\n" + "="*40)
        report.append("üìà PERFORMANCE SUMMARY")
        report.append("="*40)
        
        # T√¨m baseline performance
        baseline = next((r for r in results if "Baseline" in r.test_name), None)
        if baseline and baseline.success:
            report.append(f"Baseline Execution Time: {baseline.duration:.1f}s")
        
        # T√¨m max concurrent ƒë·∫°t ƒë∆∞·ª£c
        concurrent_tests = [r for r in results if "Concurrent" in r.test_name and r.success]
        if concurrent_tests:
            max_concurrent = max(r.metrics.get('concurrent_count', 0) for r in concurrent_tests)
            report.append(f"Max Successful Concurrent: {max_concurrent}")
        
        # Load test performance
        load_tests = [r for r in results if "Load Test" in r.test_name and r.success]
        if load_tests:
            best_load = max(load_tests, key=lambda x: x.metrics.get('deployments_per_minute', 0))
            report.append(f"Best Throughput: {best_load.metrics.get('deployments_per_minute', 0):.1f} deployments/minute")
        
        return "\n".join(report)
    
    def save_report(self, filename: str = None, results: List[TestResult] = None):
        """L∆∞u b√°o c√°o ra file"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"scalability_report_{timestamp}.txt"
        
        report = self.generate_report(results)
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"üìã Report saved to: {filename}")


# CLI Interface
@click.group()
def cli():
    """Scalability Testing CLI cho Microservices Deployment Orchestrator"""
    pass

@cli.command()
@click.option('--concurrent', '-c', default=5, help='S·ªë concurrent deployments')
def concurrent_test(concurrent):
    """Ch·∫°y concurrent deployment test"""
    tester = ScalabilityTester()
    result = tester.run_concurrent_deployment_test(concurrent)
    
    print(f"\nüìä Test Results:")
    print(f"Success: {result.success}")
    print(f"Duration: {result.duration:.1f}s")
    if result.metrics:
        for key, value in result.metrics.items():
            print(f"{key}: {value}")

@cli.command()
@click.option('--concurrent', '-c', default=5, help='Concurrent deployments per batch')
@click.option('--total', '-t', default=20, help='Total deployments')
@click.option('--interval', '-i', default=2.0, help='Interval between batches (seconds)')
def load_test(concurrent, total, interval):
    """Ch·∫°y load test"""
    tester = ScalabilityTester()
    
    config = LoadTestConfig(
        concurrent_deployments=concurrent,
        total_deployments=total,
        deployment_interval=interval,
        timeout_per_deployment=300,
        services_per_deployment=["auth-service"]
    )
    
    result = tester.run_load_test(config)
    
    print(f"\nüìä Load Test Results:")
    print(f"Success: {result.success}")
    print(f"Duration: {result.duration:.1f}s")
    if result.metrics:
        for key, value in result.metrics.items():
            if isinstance(value, float):
                print(f"{key}: {value:.2f}")
            else:
                print(f"{key}: {value}")

@cli.command()
@click.option('--max-concurrent', '-m', default=15, help='Max concurrent deployments')
@click.option('--step', '-s', default=3, help='Step size for increasing concurrent')
def stress_test(max_concurrent, step):
    """Ch·∫°y stress test"""
    tester = ScalabilityTester()
    results = tester.run_stress_test(max_concurrent, step)
    
    print(f"\nüìä Stress Test Results:")
    for result in results:
        status = "‚úÖ" if result.success else "‚ùå"
        print(f"{result.test_name}: {status} ({result.duration:.1f}s)")

@cli.command()
@click.option('--save', '-s', is_flag=True, help='L∆∞u b√°o c√°o ra file')
def full_suite(save):
    """Ch·∫°y to√†n b·ªô test suite"""
    tester = ScalabilityTester()
    results = tester.run_full_test_suite()
    
    # In b√°o c√°o
    report = tester.generate_report(results)
    print(report)
    
    # L∆∞u file n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
    if save:
        tester.save_report()

if __name__ == "__main__":
    cli() 